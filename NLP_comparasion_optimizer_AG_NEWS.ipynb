{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "kmulxuFRwfU0",
    "outputId": "889f13b7-9a25-4ae5-e65c-e9a3fd93caed"
   },
   "outputs": [],
   "source": [
    "#!pip install torch\n",
    "#!pip install torchtext==0.5.0\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GJuggEAgwfVA"
   },
   "source": [
    "\n",
    "Text Classification\n",
    "=================\n",
    "\n",
    "\n",
    "Load data with ngrams\n",
    "---------------------\n",
    "\n",
    "A bag of ngrams feature is applied to capture some partial information\n",
    "about the local word order. In practice, bi-gram or tri-gram are applied\n",
    "to provide more benefits as word groups than only one word. An example:\n",
    "\n",
    "::\n",
    "\n",
    "   \"load data with ngrams\"\n",
    "   Bi-grams results: \"load data\", \"data with\", \"with ngrams\"\n",
    "   Tri-grams results: \"load data with\", \"data with ngrams\"\n",
    "\n",
    "``TextClassification`` Dataset supports the ngrams method. By setting\n",
    "ngrams to 2, the example text in the dataset will be a list of single\n",
    "words plus bi-grams string.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "iyURII9_wfVB",
    "outputId": "f9365816-bdcc-42bc-90c6-d555178b92d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000lines [00:05, 23311.72lines/s]\n",
      "120000lines [00:09, 12665.08lines/s]\n",
      "7600lines [00:00, 12618.38lines/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "import re\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "from torchcontrib.optim import SWA\n",
    "\n",
    "#from torchtext.datasets import text_classification\n",
    "NGRAMS = 2\n",
    "import os\n",
    "if not os.path.isdir('./.data_sentiment'):\n",
    "    os.mkdir('./.data_sentiment')\n",
    "\n",
    "train_dataset, test_dataset = torchtext.datasets.AG_NEWS(\n",
    "    root='./.data_sentiment', ngrams=NGRAMS, vocab=None)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jWrWuP94wfVI"
   },
   "source": [
    "Define the model\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g8on_dNjwfVJ"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gg3BTQRGwfVN"
   },
   "source": [
    "Initiate an instance\n",
    "--------------------\n",
    "\n",
    "The AG_NEWS dataset has four labels and therefore the number of classes\n",
    "is four.\n",
    "\n",
    "   1 : World\n",
    "   2 : Sports\n",
    "   3 : Business\n",
    "   4 : Sci/Tec\n",
    "\n",
    "The vocab size is equal to the length of vocab (including single word\n",
    "and ngrams). The number of classes is equal to the number of labels,\n",
    "which is four in AG_NEWS case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8WboevFqwfVP"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
    "EMBED_DIM = 32\n",
    "NUN_CLASS = len(train_dataset.get_labels())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "omRqNmVWwfVY"
   },
   "source": [
    "Functions used to generate batch\n",
    "--------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HKFLcNLnwfVb"
   },
   "source": [
    "Since the text entries have different lengths, a custom function\n",
    "generate_batch() is used to generate data batches and offsets. The\n",
    "function is passed to ``collate_fn`` in ``torch.utils.data.DataLoader``.\n",
    "The input to ``collate_fn`` is a list of tensors with the size of\n",
    "batch_size, and the ``collate_fn`` function packs them into a\n",
    "mini-batch.\n",
    "\n",
    "The text entries in the original data batch input are packed into a list\n",
    "and concatenated as a single tensor as the input of ``nn.EmbeddingBag``.\n",
    "The offsets is a tensor of delimiters to represent the beginning index\n",
    "of the individual sequence in the text tensor. Label is a tensor saving\n",
    "the labels of individual text entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yPkYS1OfwfVd"
   },
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    label = torch.tensor([entry[0] for entry in batch])\n",
    "    text = [entry[1] for entry in batch]\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = torch.cat(text)\n",
    "    return text, offsets, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kuqKCuNNwfVj"
   },
   "source": [
    "Define functions to train the model and evaluate results.\n",
    "---------------------------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IXjhts9awfVn"
   },
   "source": [
    "We use ``DataLoader`` here to load AG_NEWS datasets and send it to the\n",
    "model for training/validation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lm7Pvc7AwfVo"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_func(sub_train_,model,optimizer,scheduler,criterion):\n",
    "\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                      collate_fn=generate_batch)\n",
    "    \n",
    "    for i, (text, offsets, cls) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        output = model(text, offsets)\n",
    "        loss = criterion(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == cls).sum().item()\n",
    "    if(scheduler != None):\n",
    "        scheduler.step()\n",
    "    \n",
    "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
    "\n",
    "def test(data_,model,optimizer,criterion):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    for text, offsets, cls in data:\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(text, offsets)\n",
    "            loss = criterion(output, cls)\n",
    "            loss += loss.item()\n",
    "            acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    return loss / len(data_), acc / len(data_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "APSZL94SwfVt"
   },
   "source": [
    "Split the dataset and run the model\n",
    "-----------------------------------\n",
    "\n",
    "Since the original AG_NEWS has no valid dataset, we split the training\n",
    "dataset into train/valid sets with a split ratio of 0.95 (train) and\n",
    "0.05 (valid). \n",
    "\n",
    "CrossEntropyLoss criterion combines nn.LogSoftmax() and nn.NLLLoss() in a single class.\n",
    "It is useful when training a classification problem with C classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "colab_type": "code",
    "id": "K511gXl6wfVu",
    "outputId": "aaa2f13e-11ff-424b-a1e4-2d4ad6b63961",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_optimizer(model,optimizer,name_file = \"new\",scheduler=None,N_EPOCHS=20):\n",
    "    \n",
    "    min_valid_loss = float('inf')\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    train_len = int(len(train_dataset) * 0.95)\n",
    "    sub_train_, sub_valid_ = \\\n",
    "        random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n",
    "\n",
    "    train_loss_tab = []\n",
    "    train_acc_tab = []\n",
    "    valid_loss_tab = []\n",
    "    valid_acc_tab = []\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "        train_loss, train_acc = train_func(sub_train_,model,optimizer,scheduler,criterion)\n",
    "        valid_loss, valid_acc = test(sub_valid_,model,optimizer,criterion)\n",
    "\n",
    "        secs = int(time.time() - start_time)\n",
    "        mins = secs / 60\n",
    "        secs = secs % 60\n",
    "\n",
    "        print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "        print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "        print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')\n",
    "        \n",
    "        train_loss_tab.append(train_loss)\n",
    "        train_acc_tab.append(train_acc)\n",
    "        valid_loss_tab.append(float(valid_loss))\n",
    "        valid_acc_tab.append(valid_acc)\n",
    "\n",
    "    \n",
    "    dict_data = {\n",
    "        'train_loss_tab':train_loss_tab,\n",
    "        'train_acc_tab':train_acc_tab,\n",
    "        'valid_loss_tab':valid_loss_tab,\n",
    "        'valid_acc_tab':valid_acc_tab\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(dict_data, columns= dict_data.keys())\n",
    "    print(df)\n",
    "    \n",
    "    path = \"D:\\\\dossier important 2020\\\\swa_gaussian-master\\\\optimizer_results\\\\\" + name_file + \".csv\"\n",
    "    \n",
    "    df.to_csv (path, index = False, header=True)\n",
    "        \n",
    "    return train_loss_tab,train_acc_tab, valid_loss_tab, valid_acc_tab\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD implements stochastic gradient descent method as optimizer. The initial\n",
    "learning rate is set to 4.0. \n",
    "\n",
    "StepLR is used here to adjust the learning rate through epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | time in 0 minutes, 25 seconds\n",
      "\tLoss: 0.0253(train)\t|\tAcc: 85.8%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 89.3%(valid)\n",
      "Epoch: 2  | time in 0 minutes, 25 seconds\n",
      "\tLoss: 0.0165(train)\t|\tAcc: 91.0%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 91.0%(valid)\n",
      "Epoch: 3  | time in 0 minutes, 24 seconds\n",
      "\tLoss: 0.0141(train)\t|\tAcc: 92.4%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 91.4%(valid)\n",
      "Epoch: 4  | time in 0 minutes, 25 seconds\n",
      "\tLoss: 0.0124(train)\t|\tAcc: 93.2%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 91.6%(valid)\n",
      "Epoch: 5  | time in 0 minutes, 23 seconds\n",
      "\tLoss: 0.0112(train)\t|\tAcc: 93.8%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 90.5%(valid)\n",
      "Epoch: 6  | time in 0 minutes, 23 seconds\n",
      "\tLoss: 0.0101(train)\t|\tAcc: 94.4%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 90.3%(valid)\n",
      "Epoch: 7  | time in 0 minutes, 24 seconds\n",
      "\tLoss: 0.0092(train)\t|\tAcc: 95.0%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 90.6%(valid)\n",
      "Epoch: 8  | time in 0 minutes, 24 seconds\n",
      "\tLoss: 0.0084(train)\t|\tAcc: 95.4%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 91.0%(valid)\n",
      "Epoch: 9  | time in 0 minutes, 24 seconds\n",
      "\tLoss: 0.0077(train)\t|\tAcc: 95.7%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 90.9%(valid)\n",
      "Epoch: 10  | time in 0 minutes, 24 seconds\n",
      "\tLoss: 0.0071(train)\t|\tAcc: 96.1%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 91.0%(valid)\n",
      "Epoch: 11  | time in 0 minutes, 24 seconds\n",
      "\tLoss: 0.0065(train)\t|\tAcc: 96.4%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 90.8%(valid)\n",
      "Epoch: 12  | time in 0 minutes, 24 seconds\n",
      "\tLoss: 0.0061(train)\t|\tAcc: 96.7%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 90.6%(valid)\n",
      "Epoch: 13  | time in 0 minutes, 24 seconds\n",
      "\tLoss: 0.0056(train)\t|\tAcc: 96.9%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 90.9%(valid)\n",
      "Epoch: 14  | time in 0 minutes, 24 seconds\n",
      "\tLoss: 0.0052(train)\t|\tAcc: 97.2%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 90.5%(valid)\n",
      "Epoch: 15  | time in 0 minutes, 24 seconds\n",
      "\tLoss: 0.0048(train)\t|\tAcc: 97.4%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 90.1%(valid)\n",
      "Epoch: 16  | time in 0 minutes, 23 seconds\n",
      "\tLoss: 0.0045(train)\t|\tAcc: 97.6%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 90.7%(valid)\n",
      "Epoch: 17  | time in 0 minutes, 23 seconds\n",
      "\tLoss: 0.0042(train)\t|\tAcc: 97.8%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 90.7%(valid)\n",
      "Epoch: 18  | time in 0 minutes, 23 seconds\n",
      "\tLoss: 0.0039(train)\t|\tAcc: 98.0%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 90.4%(valid)\n",
      "Epoch: 19  | time in 0 minutes, 24 seconds\n",
      "\tLoss: 0.0037(train)\t|\tAcc: 98.1%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 90.1%(valid)\n",
      "Epoch: 20  | time in 0 minutes, 23 seconds\n",
      "\tLoss: 0.0035(train)\t|\tAcc: 98.3%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 90.3%(valid)\n",
      "    train_loss_tab  train_acc_tab  valid_loss_tab  valid_acc_tab\n",
      "0         0.025287       0.857746        0.000044       0.893000\n",
      "1         0.016518       0.910351        0.000063       0.909833\n",
      "2         0.014058       0.923728        0.000068       0.913667\n",
      "3         0.012434       0.932404        0.000045       0.915667\n",
      "4         0.011174       0.938395        0.000035       0.904833\n",
      "5         0.010092       0.944430        0.000051       0.903333\n",
      "6         0.009188       0.949807        0.000125       0.906000\n",
      "7         0.008429       0.953754        0.000065       0.910000\n",
      "8         0.007723       0.957281        0.000026       0.909000\n",
      "9         0.007108       0.960982        0.000044       0.909667\n",
      "10        0.006547       0.964316        0.000050       0.907833\n",
      "11        0.006055       0.967018        0.000040       0.905667\n",
      "12        0.005604       0.969404        0.000079       0.909167\n",
      "13        0.005169       0.972167        0.000075       0.905500\n",
      "14        0.004814       0.974360        0.000041       0.900667\n",
      "15        0.004520       0.975833        0.000070       0.907000\n",
      "16        0.004218       0.977579        0.000077       0.906667\n",
      "17        0.003940       0.979640        0.000063       0.904167\n",
      "18        0.003725       0.980904        0.000080       0.901500\n",
      "19        0.003506       0.982711        0.000109       0.902833\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 6, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-9f6e269d49c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0moptimizer_SGD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_SGD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer_SGD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrain_loss_tab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc_tab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_loss_tab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_acc_tab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_optimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_SGD\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer_SGD\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 6, got 4)"
     ]
    }
   ],
   "source": [
    "name_file = \"SGD\"\n",
    "model_SGD = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)\n",
    "optimizer_SGD = torch.optim.SGD(model_SGD.parameters(), lr=4.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer_SGD, 1, gamma=0.9)\n",
    "train_loss_tab, train_acc_tab, valid_loss_tab, valid_acc_tab = run_optimizer(model_SGD,optimizer_SGD,name_file,scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name_file = \"Adagrad\"\n",
    "model_Adagrad = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)\n",
    "optimizer_Adagrad = torch.optim.Adagrad(model_Adagrad.parameters(), lr=4.0)\n",
    "train_loss_tab, train_acc_tab, valid_loss_tab, valid_acc_tab, test_loss, test_acc = run_optimizer(model_Adagrad,optimizer_Adagrad,name_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_file = \"SGD_scheduler_COSINE\"\n",
    "model_SGD = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)\n",
    "\n",
    "optimizer_SGD = torch.optim.SGD(model_SGD.parameters(), lr=4.0)\n",
    "\n",
    "scheduler_COSINE = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_SGD, T_max=10)\n",
    "\n",
    "train_loss_tab, train_acc_tab, valid_loss_tab, valid_acc_tab = run_optimizer(model_SGD,optimizer_SGD,name_file,scheduler_COSINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_file = \"SWA\"\n",
    "model_SWA = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)\n",
    "\n",
    "base_opt = torch.optim.SGD(model_SWA.parameters(), lr=4.0)\n",
    "optimizer_SWA = SWA(base_opt, swa_start=10, swa_freq=5, swa_lr=0.05)\n",
    "\n",
    "train_loss_tab, train_acc_tab, valid_loss_tab, valid_acc_tab = run_optimizer(model_SWA,optimizer_SWA,name_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xxh4d-gQwfV-"
   },
   "source": [
    "Test on a random news\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "N5SeVMCcwfWC",
    "outputId": "01a85667-d5d2-41fa-b07a-3cb8fa91decf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Sports news\n"
     ]
    }
   ],
   "source": [
    "ag_news_label = {1 : \"World\", 2 : \"Sports\", 3 : \"Business\", 4 : \"Sci/Tec\"}\n",
    "\n",
    "def predict(text, model, vocab, ngrams):\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor([vocab[token]\n",
    "                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() + 1\n",
    "\n",
    "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
    "    enduring the season’s worst weather conditions on Sunday at The \\\n",
    "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
    "    considering the wind and the rain was a respectable showing. \\\n",
    "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
    "    was another story. With temperatures in the mid-80s and hardly any \\\n",
    "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
    "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
    "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
    "    was even more impressive considering he’d never played the \\\n",
    "    front nine at TPC Southwind.\"\n",
    "\n",
    "vocab = train_dataset.get_vocab()\n",
    "model = model_SGD.to(\"cpu\")\n",
    "\n",
    "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, model_SGD, vocab, 2)])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Copie de text_sentiment_ngrams_tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
